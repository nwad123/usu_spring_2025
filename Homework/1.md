# Homework 1 - HPC

1. ✅ In following program example we were assuming that each call to `Compute_next_value` requires
   roughly the same amount of work in each loop iteration. For your reference:

   ```cpp
   auto my_sum = 0;
   auto my_first_i = /* ... */;
   auto my_last_i = /* ... */;

   for (auto my_i = my_first_i; my_i < my_last_i; my_i++) {
      auto my_x = Compute_next_value(/* ... */);
      my_sum += my_x;
   }
   ```

   Let's now consider instead that a call when $ i = k $ requires $ k + 1 $ units of time as the call
   when $ i = 0 $. Example: if the first call requires 1 millisecond, the second requires 2ms, the
   third requires 3ms, and so on.

   > Every process get assigned consecutive iterations in order (exactly like in the example in the
   > slides/book), i.e., process 0 will work on the first few iterations, while processor 1 will on
   > the second set of iterations, etc.

   How much time each core will spend in the loop calling the function `Compute_next_value` if
   $ n = 15 $ and $ p = 3 $? Where $n$ is the number of elements in the loop and $p$ in the number of
   available cores.

   **Answer:**

   We'll assume we aren't counting the time to actually distribute the work over the three cores, so
   we'll start by splitting the elements between each core. Since we are assuming that each loop
   iteration takes roughly the same amount of time, we'll just evenly divide the 15 elements between
   the three cores, meaning that we have 3 elements per core.

   15 elements / 3 cores = 5 elements per core

   The time is then trivial to calculate, as we know that the time can be calculated as i + 1 ms, where
   i is the number of elements processed. For example, core 0 will process elements 0 through 4, so the 
   amount of time would be `sum([0, 1, 2, 3, 4)] + 5`

   | Core | Time (ms) | Possible Elements Processed |
   | :--: | :-------: | :-------------------------: |
   |  0   |    10ms    |             0:4             |
   |  1   |    35ms    |             5:9             |
   |  2   |    60ms    |            10:14            |

2. ✅ In the previous question, the load among the cores is not balanced anymore. How would you balance
   it?

   Answer devising an algorithm (written in pseudo-code) which produces the following output:

   - An array containing the total amount of time (i.e., ms) spent on each core
   - A bi-dimensional array containing the indices of the iterations ($i$) you assign to each core
   - Finally, report a numerical example (i.e., including the content of the arrays).

   **Answer:**

   I'll give an example using python. This method does not guarantee _perfect_ balancing, but it's 
   fast to calculate and has no runtime requirement.

   ```python  
   total_time = sum(range(n)) + n 

   target_time_per_core = total_time // p

   indices = []
   for core in range(p)
       indices.append(
           [(i * p) + core for i in range(n) if (i * p) + core < n]
       )

   times = [sum(i) + len(i) for i in indices]
   ```

   For $n = 15$ and $p = 3$ we get the following result:

   `indices == [[0, 3, 6, 9, 12], [1, 4, 7, 10, 13], [2, 5, 8, 11, 14]]`

   `time = [35, 40, 45]`


3. ✅ Derive formulas for the number of receives and additions that core 0 and core 4 carry out using

   1. the original pseudo-code for a global sum, and
   2. the tree-structured global sum.

   > Assume $ n $ (number of cores) is always a power of two. Assume the initial partial sums have
   > already been computed by each core.

   Finally, report the numbers of receives and additions carried out by core 0 and core 4 (in two
   different tables) when the two methods are used with 2, 4, 16, 512 and 1024 cores.

   **Answer:**

   I'm not entirely sure what this question is asking, so I'm going to make my assumptions clear
   here. I'm assuming that each core has taken it's slice of values and calculated the partial sum
   of `compute_value` on each value, like this:

   ```cpp
   // for each core
   auto partial = 0;

   for (auto value : slice) {
      partial += compute_next_value(value);
   }

   // we are currently _here_ for each core
   ```

   Once we get the partial sum, I'm going to assume that the "original psuedo-code" for the global
   sum refers to slide 37 of the introduction slides on canvas.

   - **Core 0**

     Original psuedo-code:

     $$ Receives_{0} = n - 1 $$

     $$ Adds_{0} = n - 1 $$

     Tree-structured sum:

     $$ Receives_{0} = \log_2{(n)} $$

     $$ Adds_{0} = \log_2{(n)} $$

     | $n$  | Original-Receives | Original-Adds | Tree-Receives | Tree-Adds |
     | :--: | :---------------: | :-----------: | :-----------: | :-------: |
     |  2   |         1         |       1       |       1       |     1     |
     |  4   |         3         |       3       |       2       |     2     |
     |  16  |        15         |      15       |       4       |     4     |
     | 512  |        513        |      513      |       9       |     9     |
     | 1024 |       1023        |     1023      |      10       |    10     |

   - **Core 4**

     Original psuedo-code:

     $$ Receives_{4} = 0 $$

     $$ Adds_{4} = 0 $$

     Tree-structured sum:

     $$ Receives_{4} = min(\log_2{(n)}, 2) $$

     $$ Adds_{4} = min(\log_2{(n)}, 2) $$

     | $n$  | Original-Receives | Original-Adds | Tree-Receives | Tree-Adds |
     | :--: | :---------------: | :-----------: | :-----------: | :-------: |
     |  2   |         0         |       0       |       1       |     1     |
     |  4   |         0         |       0       |       2       |     2     |
     |  16  |         0         |       0       |       2       |     2     |
     | 512  |         0         |       0       |       2       |     2     |
     | 1024 |         0         |       0       |       2       |     2     |

4. ✅ Suppose that your professor is organizing a pizza party for all the students and need your help to
   prepare things faster and clean up afterwards.

   1. Identify tasks for students that will allow them to use "task-parallelism"
   2. How can we use "data-parallelism" to partition the work of cleaning
   3. How could you use a combination of the previous approaches?

   > The use of any type of fruit on the pizza is strongly discouraged!

   **Answer:**

   1. Tasks to be completed:

      - Setting out tables
      - Putting chairs out to go around the tables
      - Setting out plates, forks, and knives
      - Make the pizzas
      - Set the pizzas out for the students to eat
      - Put away tables
      - Throw away trash
      - Refrigerate or freeze pizza
      - Wash off tables

   2. Data parallelism:

      - Put away several chairs at once
      - Throw away multiple trash bags (or pizza boxes) at once

   3. Combination

      - Most tasks has elements that can be parallelized. For example, when setting out tables one
        student could bring several tables to the room at once, then another student could unfold them
        and set them up

5. ✅ Write-through Cache

   1. Explain how a write-through cache works.
   2. If we implement in hardware a queue in the CPU, how could this be used to improve the
      performance of a write-through cache?

   **Answer:**

   1. When data is written back into a cache from a register, the updated cache line is written back
      through successive caches back into main memory.

      For example, if you have an integer on the stack,
      `i`, and you perform the assignment `i = i * 3`, the initial value of `i` will be read from the cache
      (thus pulling the cache line containing `i` into the L1 cache) into a register, the register will be
      updated with the new value, `i * 3`, and then that will be written back into the same cache line.
      Once the L1 cache receives the updated value it will proceed to propagate the value into the L2 and
      L3 caches, and finally into main memory

   2. This could act as a buffer that wrote cache lines back into main memory. The benefits of this buffer
      is that other reads and writes could happen in the cache while the write back is occuring.

6. ✅ Recall the example involving cache reads of a two-dimensional array. For your reference:

   ```cpp
   /* First pair of loops */

   for (i = 0; i < MAX; i++)
     for (j = 0; j < MAX; j++)
       y[i] += A[i][j]*x[j];

   /* Assign y = 0 */

   /* Second pair of loops */

   for (j = 0; j < MAX; j++)
     for (i = 0; i < MAX; i++)
       y[i] += A[i][j]*x[j];
   ```

   1. How does a larger matrix affect the performance of the two pairs of nested loops?

      **Answer:**

      We'll consider each memory access of each loop separately, assuming that collisions in
      the cache are minimal (which may not be true).

      - Loop 1

        - Accesses to `y` will improve cache hits with a larger cache, because once `y[i]` is
          accessed for the first time, it will be in the cache (along with `y[i+1], y[i+2], and y[i+3]`
          for the entirety of the iteration of `j`. That means that for each cache miss of `y`
          you will get `j*4 - 1` hits on `y`. Excellent performance especially as the matrix
          grows.

        - Accesses to `A` will maintain a three to one hit/miss ratio, as we are still iterating
          through the four elements in each cache line.

          ```
          A[i][0] - miss
          A[i][1] - hit
          A[i][2] - hit
          A[i][3] - hit
          A[i][4] - miss
          ```

          etc. etc.

        - Accesses to `x` will be the same as `A`, with a three to one hit/miss ratio.

      - Loop 2

        - Accesses to `y` will be equally bad with a larger cache. Since we are accessing
          `y` with a simple stride pattern our hit/miss ratio will be three to one.

          ```
          y[0] - miss
          y[1] - hit
          y[2] - hit
          y[3] - hit
          y[4] - miss
          ```

          etc. etc.

          This means that a larger matrix will only present a linear slow-down for accesses to `y`
          related to the matrix size.

        - Accesses to `A` will also be equivalently bad, if not worse as we have very little chance
          of accessing something that was already in the cache. Here's an example:

          Smaller matrix (fits entirely in the cache):

          ```
          A[0][0] - miss
          A[1][0] - miss
          ...
          A[N][0] - miss

          ...
          A[0][1] - hit!
          ```

          With a small matrix, it's possible that if the entire matrix fits in the cache you actually
          will just have a 3/1 hit/miss ratio, like loop 1. However, as soon as the matrix gets too
          big to actually fit in the cache you immediately got a 0/1 hit/miss ratio, which will dramatically
          slow down your accesses.

          Bigger matrix (does not fit in cache):

          ```
          A[0][0] - miss
          A[1][0] - miss
          ...
          A[N][0] - miss

          ...
          A[0][1] - miss (A[0][0] was evicted earlier)
          ```

          Overall, unless the smaller matrix fit entirely in the cache you would continue to have only cache
          misses for accesses to `A`, so there would only be a linear slow-down directly related to the size
          of the matrix.

        - Accesses to `x` would actually improve their hit/miss ratio, as for each iteration of `i` you are
          accessing the same values of `j` in `x`. The hit/miss ratio of `x` (assuming it never gets evicted)
          is `i*4 - 1 / 1`.

      Overall, the second loop would be quite a bit slower because of the poor access pattern to `A`.

   2. How does a larger cache affect the performance of the two pairs of nested loops?

      **Answer:**

      For the first loop the only potential performance increase could come from the `x` vector staying
      in the cache for longer. If on each iteration of `i` we didn't need to refetch `x[0]` into the
      that would improve our hit/miss ratio. Every other element is only accessed once, so we wouldn't
      see any other benefit for our `y` accesses or our `A` accesses.

      For the second loop if the cache was large enough and `A` was small enough to fit in the cache then
      we would see similar hit/miss ratios to loop 1.

      Overall however, if we can fit all the values of `y`, `A`, and `x` in the cache, all of a sudden
      loop 1 and loop 2 would exhibit the same hit/miss ratio, as everything would be in the cache. For
      any significantly sized array or matrix however, this is probably pretty unlikely.

   3. What happens if MAX = 8 and the cache can store four lines? (Hint: draw a table representing the
      cache lines, assume each cache line contains 4 elements of the array)

      **Answer:**

      I'm going to assume that we don't need to provide the state of the cache for each entire loop,
      as that would be 128 tables representing each possible state the cache could be in. Also, we
      aren't given addresses so deciding which line an access should go into will be slightly arbitrary
      for this problem.

      - Loop 1, first iteration:

        ```
        // ...
        y[0] = A[0][0] * x[0]
        ```

        | Line | Element 0 | Element 1 | Element 2 | Element 3 |
        | :--: | :-------: | :-------: | :-------: | :-------: |
        |  0   | `A[0][0]` | `A[0][1]` | `A[0][2]` | `A[0][3]` |
        |  1   |  `x[0]`   |  `x[1]`   |  `x[2]`   |  `x[3]`   |
        |  2   |  `y[0]`   |  `y[1]`   |  `y[2]`   |  `y[3]`   |
        |  3   |           |           |           |           |

      - Loop 1, second iteration:

        ```
        // ...
        y[0] = A[0][1] * x[1]
        ```

        | Line | Element 0 | Element 1 | Element 2 | Element 3 |
        | :--: | :-------: | :-------: | :-------: | :-------: |
        |  0   | `A[0][0]` | `A[0][1]` | `A[0][2]` | `A[0][3]` |
        |  1   |  `x[0]`   |  `x[1]`   |  `x[2]`   |  `x[3]`   |
        |  2   |  `y[0]`   |  `y[1]`   |  `y[2]`   |  `y[3]`   |
        |  3   |           |           |           |           |

        _Notice that because the access pattern is good, the cache actually does not change, because
        everything we need is already present in the cache_

      - Loop 1, fifth iteration (first iteration where an eviction happens):

        ```
        // ...
        y[0] = A[0][4] * x[4]
        ```

        | Line | Element 0 | Element 1 | Element 2 | Element 3 |         |
        | :--: | :-------: | :-------: | :-------: | :-------: | :-----: |
        |  0   |  `x[4]`   |  `x[5]`   |  `x[6]`   |  `x[7]`   | Evicted |
        |  1   |  `x[0]`   |  `x[1]`   |  `x[2]`   |  `x[3]`   |         |
        |  2   |  `y[0]`   |  `y[1]`   |  `y[2]`   |  `y[3]`   |         |
        |  3   | `A[0][4]` | `A[0][5]` | `A[0][6]` | `A[0][7]` |         |

        When `A[0][4]` is fetched, there is an empty line in the cache so it's pulled into that
        line. Then, then `x[4]` needs to be fetched to perform the multiplication, there aren't
        any empty lines so the cache has to evict a line. Assuming a LRU replacement policy, it's
        likely that line 0 will be evicted, and `x[4]` will be pulled into that line. Since `y[0]`
        is still in the cache, no new pull for `y` needs to happen so that line is unchanged.

      Since `y` is accessed every loop iteration it's likely to just remain in the cache without
      being evicted very often, likely meaning that we will actually achieve the hit/miss ratio
      we calculated in the previous part of this question. The cache will alternate pulling in new
      lines of `x` and `A`, overwriting previous lines, again likely reaching the hit/miss ratio
      specified in the previous part of this question.

      - Loop 2, first iteration:

        ```
        // ...
        y[0] = A[0][0] * x[0]
        ```

        | Line | Element 0 | Element 1 | Element 2 | Element 3 |
        | :--: | :-------: | :-------: | :-------: | :-------: |
        |  0   | `A[0][0]` | `A[0][1]` | `A[0][2]` | `A[0][3]` |
        |  1   |  `x[0]`   |  `x[1]`   |  `x[2]`   |  `x[3]`   |
        |  2   |  `y[0]`   |  `y[1]`   |  `y[2]`   |  `y[3]`   |
        |  3   |           |           |           |           |

      - Loop 2, second iteration:

        ```
        // ...
        y[1] = A[1][0] * x[0]
        ```

        | Line | Element 0 | Element 1 | Element 2 | Element 3 |
        | :--: | :-------: | :-------: | :-------: | :-------: |
        |  0   | `A[0][0]` | `A[0][1]` | `A[0][2]` | `A[0][3]` |
        |  1   |  `x[0]`   |  `x[1]`   |  `x[2]`   |  `x[3]`   |
        |  2   |  `y[0]`   |  `y[1]`   |  `y[2]`   |  `y[3]`   |
        |  3   | `A[1][0]` | `A[1][1]` | `A[1][2]` | `A[1][3]` |

        _Notice that unlike loop 1, we already have to pull in a new line for `A`_

      - Loop 2, third iteration (first eviction):

        ```
        // ...
        y[2] = A[2][0] * x[0]
        ```

        | Line | Element 0 | Element 1 | Element 2 | Element 3 |          |
        | :--: | :-------: | :-------: | :-------: | :-------: | :------- |
        |  0   | `A[2][0]` | `A[2][1]` | `A[2][2]` | `A[2][3]` | Eviction |
        |  1   |  `x[0]`   |  `x[1]`   |  `x[2]`   |  `x[3]`   |          |
        |  2   |  `y[0]`   |  `y[1]`   |  `y[2]`   |  `y[3]`   |          |
        |  3   | `A[1][0]` | `A[1][1]` | `A[1][2]` | `A[1][3]` |          |

        Because we again have to pull in another new line for the newest access of `A`, we will
        evict the first line of `A` we pulled in, even though we only accessed one element from
        that line.

      - Loop 2, fifth iteration (multiple eviction):

        ```
        // ...
        y[4] = A[4][0] * x[0]
        ```

        | Line | Element 0 | Element 1 | Element 2 | Element 3 |          |
        | :--: | :-------: | :-------: | :-------: | :-------: | :------- |
        |  0   | `A[4][0]` | `A[4][1]` | `A[4][2]` | `A[4][3]` | Eviction |
        |  1   |  `x[0]`   |  `x[1]`   |  `x[2]`   |  `x[3]`   |          |
        |  2   |  `y[0]`   |  `y[1]`   |  `y[2]`   |  `y[3]`   |          |
        |  3   |  `y[4]`   |  `y[5]`   |  `y[6]`   |  `y[7]`   | Eviction |

      Overall, loop 2 will not use the cache very inefficiently, and will cause lots of evictions
      for data that it hasn't even used yet.

7. ✅ Describe simultaneous multithreading (SMT) and provide an example of a workload that would
   benefit from it.

   **Answer:**

   SMT is a hardware construct where two logical CPU cores share caches but have different compute
   units. This is useful for workloads where two threads operate on the same data, because it
   provides better physical locality for the threads if they share the same cache.

   An example of when this is useful could be a mandelbrot image generator where two threads are
   working on generating the image. The buffer of pixels could be stored in cache, and each logical
   core could calculate the value for different pixels in the array. Since they share a cache, there
   would not be penalties of cache coherence and there would be benefits of data locality.

8. ✅ Provide an example of parallel program which output is non-deterministic and why. Explain how you
   would change the program to enforce a deterministic output.

   **Answer:**

   Demonstration: [Compiler Explorer](https://godbolt.org/z/16qYqr5Ts)

   ```cpp
   #include <thread>
   #include <chrono>
   #include <fmt/format.h>

   using std::thread;

   auto simulate_work() -> void
   {
       std::this_thread::sleep_for (std::chrono::seconds(1));
   }

   auto non_deterministic() -> void
   {
       int x{0};

       auto inc = [&x]() {
           simulate_work();
           x = 5;
       };

       auto dec = [&x]() {
           simulate_work();
           x = 10;
       };

       auto t1 = thread(inc);
       auto t2 = thread(dec);

       t1.join();
       t2.join();

       fmt::println("{}", x);
   }

   auto main() -> int
   {
       // will print out different answers
       non_deterministic();
       non_deterministic();
       non_deterministic();
   }
   ```

   To make this deterministic first you'd have to determine the order you wanted, then you could either use
   some form of synchronization primitive (like a mutex) or you could use atomics or you could use a different
   algorithm.

9. ✅ Explain the difference between Partitioned Global Address Space and Message Passing languages for
   distributed memory systems. In which type of application would you use one or the other?

   **Answer:**

   Partitioned global address space programs provide an abstraction of a single unified global address, even 
   if the memory is actually spread out over many different computers. Message passing languages provide an 
   abstraction of messages that are sent / received by each process.

   Partitioned global address space systems are good for programs that can easily operate on different memory 
   regions natively. For example, a fluid simulation solver can perform steps in each cell region of the fluid 
   simulation without needing to know what is happening in the other regions. After doing this parallel update, 
   the simulation will need to be solved which will involve coordination between the cells. Also, partioned global 
   address space systems are good for systems that need a large chunk of memory, such as a fluid simulation 
   system.

   Message passing systems are good for programs that can perform lots of operations on their own in their own 
   memory space, only sharing information when needed. Also, message passing can be used for fault tolerant 
   systems as in the case of the Erlang programming language.

10. ✅ Suppose that a vector processor has a memory system in which it takes 10 cycles to load a single
    64-bit word from memory. How many memory banks are needed so that a stream of loads can, on average,
    require only one cycle per load?

    **Answer:**

    10

# Homework 1 - HPC

1. ✅ In following program example we were assuming that each call to `Compute_next_value` requires
   roughly the same amount of work in each loop iteration. For your reference:

   ```cpp
   auto my_sum = 0;
   auto my_first_i = /* ... */;
   auto my_last_i = /* ... */;

   for (auto my_i = my_first_i; my_i < my_last_i; my_i++) {
      auto my_x = Compute_next_value(/* ... */);
      my_sum += my_x;
   }
   ```

   Let's now consider instead that a call when $ i = k $ requires $ k + 1 $ units of time as the call
   when $ i = 0 $. Example: if the first call requires 1 millisecond, the second requires 2ms, the
   third requires 3ms, and so on.

   > Every process get assigned consecutive iterations in order (exactly like in the example in the
   > slides/book), i.e., process 0 will work on the first few iterations, while processor 1 will on
   > the second set of iterations, etc.

   How much time each core will spend in the loop calling the function `Compute_next_value` if
   $ n = 15 $ and $ p = 3 $? Where $n$ is the number of elements in the loop and $p$ in the number of
   available cores.

   **Answer:**

   We'll assume we aren't counting the time to actually distribute the work over the three cores, so
   we'll start by splitting the elements between each core. Since we are assuming that each loop
   iteration takes roughly the same amount of time, we'll just evenly divide the 15 elements between
   the three cores, meaning that we have 3 elements per core.

   15 elements / 3 cores = 5 elements per core

   The time is then trivial to calculate, as we know that the time can be calculated as i + 1 ms, where
   i is the number of elements processed. That means each core will take 6 ms to complete, not accounting
   for setup time.

   | Core | Time (ms) | Possible Elements Processed |
   | :--: | :-------: | :-------------------------: |
   |  0   |    6ms    |             0:4             |
   |  1   |    6ms    |             5:9             |
   |  2   |    6ms    |            10:14            |

2. ✅ In the previous question, the load among the cores is not balanced anymore. How would you balance
   it?

   Answer devising an algorithm (written in pseudo-code) which produces the following output:

   - An array containing the total amount of time (i.e., ms) spent on each core
   - A bi-dimensional array containing the indices of the iterations ($i$) you assign to each core
   - Finally, report a numerical example (i.e., including the content of the arrays).

   **Answer:**

   ```
   function balance_cores()
      // index of current iteration
      i: integer := 0

      // iterations assigned to each core
      // e.g. iterations assigned core 0 corresponds to assignments[0]
      assignments: int[int[]] := [[], [], []]

      // start a timer
      t: timer
      t.start()

      // keep track of the last timestamp that a core was assigned work on
      // this will help us keep track of how much time is spent on each core
      //
      // each element in the array corresponds to a core, e.g. last_assigned[0]
      // is the time that core 0 was last given an assignment
      last_assigned: time[] = [0, 0, 0]

      // this array keeps track of the sum of time for each core. As before,
      // total_time[0] corresponds to the time for core 0, and so on and so
      // forth
      total_time: time[] := [0, 0, 0]

      // seed initial work to each core and set the last_assigned time for each
      // core
      for j in [0..p]:
         last_assigned[j] = t.now()
         core[j].assign(i)
         i += 1

      // now cycle through the cores and when one is available assign it new work
      while i != n:
         for j in [0..p]:
            if !core[j].is_done() then continue

            total_time[j] += t.now() - last_assigned[j]
            last_assigned[j] = t.now()
            core[j].assign(i)
            i += 1

            if i >= n then break

      // now all the work has been assigned (i == n) so wait for all the
      // cores to finish and total up their time
      //
      // we also need to keep track of which cores we are completely done with,
      // so we will keep a list of cores that we have finished working on
      cores_left = [0..p]

      while !cores_left.empty():
         for j in cores_left:
            if !core[j].is_done() then continue

            total_time[j] += t.now() - last_assigned[j]
            cores_left.erase(j)
   ```

3. Derive formulas for the number of receives and additions that core 0 and core 4 carry out using

   1. the original pseudo-code for a global sum, and
   2. the tree-structured global sum.

   > Assume $ n $ (number of cores) is always a power of two. Assume the initial partial sums have
   > already been computed by each core.

   Finally, report the numbers of receives and additions carried out by core 0 and core 4 (in two
   different tables) when the two methods are used with 2, 4, 16, 512 and 1024 cores.

   **Answer:**

4. ✅ Suppose that your professor is organizing a pizza party for all the students and need your help to
   prepare things faster and clean up afterwards.

   1. Identify tasks for students that will allow them to use "task-parallelism"
   2. How can we use "data-parallelism" to partition the work of cleaning
   3. How could you use a combination of the previous approaches?

   > The use of any type of fruit on the pizza is strongly discouraged!

   **Answer:**

   1. Tasks to be completed:

      - Setting out tables
      - Putting chairs out to go around the tables
      - Setting out plates, forks, and knives
      - Make the pizzas
      - Set the pizzas out for the students to eat
      - Put away tables
      - Throw away trash
      - Refrigerate or freeze pizza
      - Wash off tables

   2. Data parallelism:

      - Put away several chairs at once
      - Throw away multiple trash bags (or pizza boxes) at once

   3. Combination

      - Most tasks has elements that can be parallelized. For example, when setting out tables one
        student could bring several tables to the room at once, then another student could unfold them
        and set them up

5. ✅ Write-through Cache

   1. Explain how a write-through cache works.
   2. If we implement in hardware a queue in the CPU, how could this be used to improve the
      performance of a write-through cache?

   **Answer:**

   1. When data is written back into a cache from a register, the updated cache line is written back
      through successive caches back into main memory.

      For example, if you have an integer on the stack,
      `i`, and you perform the assignment `i = i * 3`, the initial value of `i` will be read from the cache
      (thus pulling the cache line containing `i` into the L1 cache) into a register, the register will be
      updated with the new value, `i * 3`, and then that will be written back into the same cache line.
      Once the L1 cache receives the updated value it will proceed to propagate the value into the L2 and
      L3 caches, and finally into main memory

   2. This could act as a buffer that wrote cache lines back into main memory. The benefits of this buffer
      is that other reads and writes could happen in the cache while the write back is occuring.

6. ✅ Recall the example involving cache reads of a two-dimensional array. For your reference:

   ```cpp
   /* First pair of loops */

   for (i = 0; i < MAX; i++)
     for (j = 0; j < MAX; j++)
       y[i] += A[i][j]*x[j];

   /* Assign y = 0 */

   /* Second pair of loops */

   for (j = 0; j < MAX; j++)
     for (i = 0; i < MAX; i++)
       y[i] += A[i][j]*x[j];
   ```

   1. How does a larger matrix affect the performance of the two pairs of nested loops?

      **Answer:**

      We'll consider each memory access of each loop separately, assuming that collisions in
      the cache are minimal (which may not be true).

      - Loop 1

        - Accesses to `y` will improve cache hits with a larger cache, because once `y[i]` is
          accessed for the first time, it will be in the cache (along with `y[i+1], y[i+2], and y[i+3]`
          for the entirety of the iteration of `j`. That means that for each cache miss of `y`
          you will get `j*4 - 1` hits on `y`. Excellent performance especially as the matrix
          grows.

        - Accesses to `A` will maintain a three to one hit/miss ratio, as we are still iterating
          through the four elements in each cache line.

          ```
          A[i][0] - miss
          A[i][1] - hit
          A[i][2] - hit
          A[i][3] - hit
          A[i][4] - miss
          ```

          etc. etc.

        - Accesses to `x` will be the same as `A`, with a three to one hit/miss ratio.

      - Loop 2

        - Accesses to `y` will be equally bad with a larger cache. Since we are accessing
          `y` with a simple stride pattern our hit/miss ratio will be three to one.

          ```
          y[0] - miss
          y[1] - hit
          y[2] - hit
          y[3] - hit
          y[4] - miss
          ```

          etc. etc.

          This means that a larger matrix will only present a linear slow-down for accesses to `y`
          related to the matrix size.

        - Accesses to `A` will also be equivalently bad, if not worse as we have very little chance
          of accessing something that was already in the cache. Here's an example:

          Smaller matrix (fits entirely in the cache):

          ```
          A[0][0] - miss
          A[1][0] - miss
          ...
          A[N][0] - miss

          ...
          A[0][1] - hit!
          ```

          With a small matrix, it's possible that if the entire matrix fits in the cache you actually
          will just have a 3/1 hit/miss ratio, like loop 1. However, as soon as the matrix gets too
          big to actually fit in the cache you immediately got a 0/1 hit/miss ratio, which will dramatically
          slow down your accesses.

          Bigger matrix (does not fit in cache):

          ```
          A[0][0] - miss
          A[1][0] - miss
          ...
          A[N][0] - miss

          ...
          A[0][1] - miss (A[0][0] was evicted earlier)
          ```

          Overall, unless the smaller matrix fit entirely in the cache you would continue to have only cache
          misses for accesses to `A`, so there would only be a linear slow-down directly related to the size
          of the matrix.

        - Accesses to `x` would actually improve their hit/miss ratio, as for each iteration of `i` you are
          accessing the same values of `j` in `x`. The hit/miss ratio of `x` (assuming it never gets evicted)
          is `i*4 - 1 / 1`.

      Overall, the second loop would be quite a bit slower because of the poor access pattern to `A`.

   2. How does a larger cache affect the performance of the two pairs of nested loops?

      **Answer:**

      For the first loop the only potential performance increase could come from the `x` vector staying
      in the cache for longer. If on each iteration of `i` we didn't need to refetch `x[0]` into the
      that would improve our hit/miss ratio. Every other element is only accessed once, so we wouldn't
      see any other benefit for our `y` accesses or our `A` accesses.

      For the second loop if the cache was large enough and `A` was small enough to fit in the cache then
      we would see similar hit/miss ratios to loop 1.

      Overall however, if we can fit all the values of `y`, `A`, and `x` in the cache, all of a sudden
      loop 1 and loop 2 would exhibit the same hit/miss ratio, as everything would be in the cache. For
      any significantly sized array or matrix however, this is probably pretty unlikely.

   3. What happens if MAX = 8 and the cache can store four lines? (Hint: draw a table representing the
      cache lines, assume each cache line contains 4 elements of the array)

      **Answer:**

      I'm going to assume that we don't need to provide the state of the cache for each entire loop,
      as that would be 128 tables representing each possible state the cache could be in. Also, we
      aren't given addresses so deciding which line an access should go into will be slightly arbitrary
      for this problem.

      - Loop 1, first iteration:

        ```
        // ...
        y[0] = A[0][0] * x[0]
        ```

        | Line | Element 0 | Element 1 | Element 2 | Element 3 |
        | :--: | :-------: | :-------: | :-------: | :-------: |
        |  0   | `A[0][0]` | `A[0][1]` | `A[0][2]` | `A[0][3]` |
        |  1   |  `x[0]`   |  `x[1]`   |  `x[2]`   |  `x[3]`   |
        |  2   |  `y[0]`   |  `y[1]`   |  `y[2]`   |  `y[3]`   |
        |  3   |           |           |           |           |

      - Loop 1, second iteration:

        ```
        // ...
        y[0] = A[0][1] * x[1]
        ```

        | Line | Element 0 | Element 1 | Element 2 | Element 3 |
        | :--: | :-------: | :-------: | :-------: | :-------: |
        |  0   | `A[0][0]` | `A[0][1]` | `A[0][2]` | `A[0][3]` |
        |  1   |  `x[0]`   |  `x[1]`   |  `x[2]`   |  `x[3]`   |
        |  2   |  `y[0]`   |  `y[1]`   |  `y[2]`   |  `y[3]`   |
        |  3   |           |           |           |           |

        _Notice that because the access pattern is good, the cache actually does not change, because
        everything we need is already present in the cache_

      - Loop 1, fifth iteration (first iteration where an eviction happens):

        ```
        // ...
        y[0] = A[0][4] * x[4]
        ```

        | Line | Element 0 | Element 1 | Element 2 | Element 3 |         |
        | :--: | :-------: | :-------: | :-------: | :-------: | :-----: |
        |  0   |  `x[4]`   |  `x[5]`   |  `x[6]`   |  `x[7]`   | Evicted |
        |  1   |  `x[0]`   |  `x[1]`   |  `x[2]`   |  `x[3]`   |         |
        |  2   |  `y[0]`   |  `y[1]`   |  `y[2]`   |  `y[3]`   |         |
        |  3   | `A[0][4]` | `A[0][5]` | `A[0][6]` | `A[0][7]` |         |

        When `A[0][4]` is fetched, there is an empty line in the cache so it's pulled into that
        line. Then, then `x[4]` needs to be fetched to perform the multiplication, there aren't
        any empty lines so the cache has to evict a line. Assuming a LRU replacement policy, it's
        likely that line 0 will be evicted, and `x[4]` will be pulled into that line. Since `y[0]`
        is still in the cache, no new pull for `y` needs to happen so that line is unchanged.

      Since `y` is accessed every loop iteration it's likely to just remain in the cache without
      being evicted very often, likely meaning that we will actually achieve the hit/miss ratio
      we calculated in the previous part of this question. The cache will alternate pulling in new
      lines of `x` and `A`, overwriting previous lines, again likely reaching the hit/miss ratio
      specified in the previous part of this question.

      - Loop 2, first iteration:

        ```
        // ...
        y[0] = A[0][0] * x[0]
        ```

        | Line | Element 0 | Element 1 | Element 2 | Element 3 |
        | :--: | :-------: | :-------: | :-------: | :-------: |
        |  0   | `A[0][0]` | `A[0][1]` | `A[0][2]` | `A[0][3]` |
        |  1   |  `x[0]`   |  `x[1]`   |  `x[2]`   |  `x[3]`   |
        |  2   |  `y[0]`   |  `y[1]`   |  `y[2]`   |  `y[3]`   |
        |  3   |           |           |           |           |

      - Loop 2, second iteration:

        ```
        // ...
        y[1] = A[1][0] * x[0]
        ```

        | Line | Element 0 | Element 1 | Element 2 | Element 3 |
        | :--: | :-------: | :-------: | :-------: | :-------: |
        |  0   | `A[0][0]` | `A[0][1]` | `A[0][2]` | `A[0][3]` |
        |  1   |  `x[0]`   |  `x[1]`   |  `x[2]`   |  `x[3]`   |
        |  2   |  `y[0]`   |  `y[1]`   |  `y[2]`   |  `y[3]`   |
        |  3   | `A[1][0]` | `A[1][1]` | `A[1][2]` | `A[1][3]` |

        _Notice that unlike loop 1, we already have to pull in a new line for `A`_

      - Loop 2, third iteration (first eviction):

        ```
        // ...
        y[2] = A[2][0] * x[0]
        ```

        | Line | Element 0 | Element 1 | Element 2 | Element 3 |          |
        | :--: | :-------: | :-------: | :-------: | :-------: | :------- |
        |  0   | `A[2][0]` | `A[2][1]` | `A[2][2]` | `A[2][3]` | Eviction |
        |  1   |  `x[0]`   |  `x[1]`   |  `x[2]`   |  `x[3]`   |          |
        |  2   |  `y[0]`   |  `y[1]`   |  `y[2]`   |  `y[3]`   |          |
        |  3   | `A[1][0]` | `A[1][1]` | `A[1][2]` | `A[1][3]` |          |

        Because we again have to pull in another new line for the newest access of `A`, we will
        evict the first line of `A` we pulled in, even though we only accessed one element from
        that line.

      - Loop 2, fifth iteration (multiple eviction):

        ```
        // ...
        y[4] = A[4][0] * x[0]
        ```

        | Line | Element 0 | Element 1 | Element 2 | Element 3 |          |
        | :--: | :-------: | :-------: | :-------: | :-------: | :------- |
        |  0   | `A[4][0]` | `A[4][1]` | `A[4][2]` | `A[4][3]` | Eviction |
        |  1   |  `x[0]`   |  `x[1]`   |  `x[2]`   |  `x[3]`   |          |
        |  2   |  `y[0]`   |  `y[1]`   |  `y[2]`   |  `y[3]`   |          |
        |  3   |  `y[4]`   |  `y[5]`   |  `y[6]`   |  `y[7]`   | Eviction |

      Overall, loop 2 will not use the cache very inefficiently, and will cause lots of evictions
      for data that it hasn't even used yet.
